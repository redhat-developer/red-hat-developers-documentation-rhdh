:_mod-docs-content-type: CONCEPT

[id="con-llm-requirements_{context}"]
= Large language model (LLM) requirements

{ls-short} follows a _Bring Your Own Model_ approach. This model means that to function, {ls-short} requires access to a large language model (LLM) which you must provide. An LLM is a type of generative AI that interprets natural language and generates human-like text or audio responses. When an LLM is used as a virtual assistant, the LLM can interpret questions and provide answers in a conversational manner.

LLMs are usually provided by a service or server. Because {ls-short} does not provide an LLM for you, you must configure your preferred LLM provider during installation. You can configure the underlying Llama Stack server to integrate with a number of LLM `providers` that offer compatibility with the OpenAI API including the following LLMs:

* OpenAI (cloud-based inference service)
* {rhoai-brand-name} (enterprise model builder & inference server)
* {rhel} AI (enterprise inference server)
* Ollama (popular desktop inference server)
* vLLM (popular enterprise inference server)
