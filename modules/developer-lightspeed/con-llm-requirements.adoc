:_mod-docs-content-type: CONCEPT

[id="con-llm-requirements_{context}"]
= Large language model (LLM) requirements

{ls-short} follows a _Bring Your Own Model_ approach. This model means that to function, {ls-short} requires access to a large language model (LLM) which you must provide. An LLM is a type of generative AI that interprets natural language and generates human-like text or audio responses. When an LLM is used as a virtual assistant, the LLM can interpret questions and provide answers in a conversational manner.

LLMs are usually provided by a service or server. Since {ls-short} does not provide an LLM for you, you must configure your preferred LLM provider during installation.
You can use {ls-short} with a number of LLM providers that offer the OpenAI API interface including the following LLMS:

* OpenAI (cloud-based inference service)
* Red Hat OpenShift AI (enterprise model builder & inference server)
* Red Hat Enterprise Linux AI (enterprise inference server)
* Ollama (popular desktop inference server)
* vLLM (popular enterprise inference server)
