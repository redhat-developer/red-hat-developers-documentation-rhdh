:_mod-docs-content-type: PROCEDURE

[id="proc-installing-and-configuring-lightspeed_{context}"]
= Installing and configuring {ls-brand-name}

{ls-short} includes three main components that work together to provide virtual assistant (chat) functionality to your developers.

* Llama Stack server (container sidecar):: This server, based on open source Llama Stack, operates as the main gateway to your LLM inferencing provider for chat services. Its modular nature allows you to integrate other services, such as the Model Context Protocol (MCP). You must integrate your LLM provider with the Llama Stack server to support the chat functionality. This dependency on external LLM providers is called *Bring Your Own Model* (BYOM).

* {lcs-name} (container sidecar):: This service, based on the open source Lightspeed Core, enables features that complement the Llama Stack server, including maintaining chat history and gathering user feedback.

* {ls-short} (dynamic plugins):: These plugins are required to enable the {ls-short} user interface within your {product-very-short} instance.

Configuring these components to initialise correctly and communicate with each other is essential in order to provide {ls-short} to your users. 

[NOTE]
====
If you have already installed the previous {ls-short} (Developer Preview) with Road-Core Service (RCS), you must remove the previous {ls-short} configurations and settings and reinstall.
This step is necessary as {ls-short} has a new architecture. In the previous release, {ls-short} required the use of the {rcs-name} as a sidecar container for interfacing with LLM providers. The updated architecture removes and replaces RCS with the new {lcs-name} and Llama Stack server, and requires new configurations for the plugins, volumes, containers, and secrets.
====

.Prerequisites
* You are logged into your {ocp-short} account.
* You have an {product-very-short} instance installed either of the following ways:
** {installing-on-ocp-book-link}#assembly-install-rhdh-ocp-operator[Using the Operator]
** {installing-on-ocp-book-link}#assembly-install-rhdh-ocp-helm[Using the Helm chart]

.Procedure

You must install and configure the {ls-short}, the {lcs-name}, and Llama Stack containers manually.

. Create the {lcs-short} ConfigMap (`lightspeed-stack.yaml`).
+
[IMPORTANT]
====
You must name the {lcs-short} ConfigMap as `lightspeed-stack.yaml`.
====
.. In the {ocp-short} web console, go to your {product-very-short} instance and select the *ConfigMaps* tab.
.. Click *Create ConfigMaps*.
.. From the *Create ConfigMap* page, select the *YAML view* option in *Configure via*, and edit the file as shown in the following example:
+
[source,yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
 name: Lightspeed Core Service (LCS)
service:
  host: 0.0.0.0
  port: 8080
  auth_enabled: false
  workers: 1
  color_log: true
  access_log: true
llama_stack:
  use_as_library_client: false
  url: http://localhost:8321
user_data_collection:
  feedback_enabled: true
  feedback_storage: "/tmp/data/feedback"
  transcripts_enabled: true
  transcripts_storage: "/tmp/data/transcripts"
authentication:
  module: "noop"
conversation_cache:
  type: "sqlite"
  sqlite:
    db_path: "./lcs_cache.db"
mcp_servers:
  - name: mcp::backstage
    provider_id: model-context-protocol
    url: https://<RHDH_HOST>/api/mcp-actions/v1
----
where:

`mcp_servers`:: Optional: Set this configuration if you want to integrate MCP. See {model-context-protocol-link}[{model-context-protocol-title}].
`mcp_servers:name`:: This value must match the entry in the {ls-short} app config file for MCP servers.
`model-context-protocol`:: This is the tool runtime provider defined and configured in the llama-stack `run.yaml` configuration for use in {lcs-short}.

.. Click *Create*.

. Create a Llama Stack ConfigMap.
.. In the {ocp-short} web console, go to your {product-very-short} instance and select the *ConfigMaps* tab.
.. Click *Create ConfigMaps*.
.. Skip this step if you have configured  the Llama Stack image (quay.io/redhat-ai-dev/llama-stack) in your CR: From the *Create ConfigMap* page, select the *YAML view* option in *Configure via*, and edit the file as shown in the following example:
+
[source,yaml]
----
version: '2'
image_name: minimal-viable-llama-stack-configuration

apis:
  - agents
  - datasetio
  - eval
  - inference
  - post_training
  - safety
  - scoring
  - telemetry
  - tool_runtime
  - vector_io
benchmarks: []
container_image: null
datasets: []
external_providers_dir: "/app-root/config/providers.d"
inference_store:
  db_path: .llama/distributions/ollama/inference_store.db
  type: sqlite
logging: null
metadata_store:
  db_path: .llama/distributions/ollama/registry.db
  namespace: null
  type: sqlite
models:
- model_id: sentence-transformers/all-mpnet-base-v2
  metadata:
      embedding_dimension: 768
  model_type: embedding
  provider_id: sentence-transformers
  provider_model_id: "/app-root/embeddings_model"
providers:
  agents:
  - config:
      persistence_store:
        db_path: .llama/distributions/ollama/agents_store.db
        namespace: null
        type: sqlite
      responses_store:
        db_path: .llama/distributions/ollama/responses_store.db
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  datasetio:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/huggingface_datasetio.db
        namespace: null
        type: sqlite
    provider_id: huggingface
    provider_type: remote::huggingface
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/localfs_datasetio.db
        namespace: null
        type: sqlite
    provider_id: localfs
    provider_type: inline::localfs
  eval:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/meta_reference_eval.db
        namespace: null
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  inference:
    - provider_id: vllm
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL}
        api_token: ${env.VLLM_API_KEY}
        max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
        tls_verify: ${env.VLLM_TLS_VERIFY:=true}
    - provider_id: sentence-transformers
      provider_type: inline::sentence-transformers
      config: {}
  post_training:
  - config:
      checkpoint_format: huggingface
      device: cpu
      distributed_backend: null
      dpo_output_dir: "."
    provider_id: huggingface
    provider_type: inline::huggingface
  safety:
    - config:
        excluded_categories: []
      provider_id: llama-guard
      provider_type: inline::llama-guard
    - provider_id: lightspeed_question_validity
      provider_type: inline::lightspeed_question_validity
      config:
        model_id: ${env.VALIDATION_PROVIDER:=vllm}/${env.VALIDATION_MODEL_NAME}
        model_prompt: |-
          Instructions:

          You area question classification tool. You are an expert in the following categories:
          - Backstage
          - Red Hat Developer Hub (RHDH)
          - Kubernetes
          - Openshift
          - CI/CD
          - GitOps
          - Pipelines
          - Developer Portals
          - Deployments
          - Software Catalogs
          - Software Templates
          - Tech Docs
          
          Your job is to determine if a user's question is related to the categories you are an expert in. If the question is related to those categories, \
          or any features that may be related to those categories, you will answer with ${allowed}.

          If a question is not related to your expert categories, answer with ${rejected}.

          You do not need to explain your answer.

          Below are some example questions:
          Example Question:
          Why is the sky blue?
          Example Response:
          ${rejected}

          Example Question:
          Can you help configure my cluster to automatically scale?
          Example Response:
          ${allowed}

          Example Question:
          How do I create import an existing software template in Backstage?
          Example Response:
          ${allowed}

          Example Question:
          How do I accomplish a task in RHDH?
          Example Response:
          ${allowed}

          Example Question:
          How do I explore a component in RHDH catalog?
          Example Response:
          ${allowed}

          Example Question:
          How can I integrate GitOps into my pipeline?
          Example Response:
          ${allowed}

          Question:
          ${message}
          Response:
        invalid_question_response: |-
          Hi, I'm the Red Hat Developer Hub Lightspeed assistant, I can help you with questions about Red Hat Developer Hub or Backstage.
          Please ensure your question is about these topics, and feel free to ask again!
  scoring:
  - config: {}
    provider_id: basic
    provider_type: inline::basic
  - config: {}
    provider_id: llm-as-judge
    provider_type: inline::llm-as-judge
  - config:
      openai_api_key: '********'
    provider_id: braintrust
    provider_type: inline::braintrust
  telemetry:
  - config:
      service_name: 'lightspeed-stack-telemetry'
      sinks: sqlite
      sqlite_db_path: .llama/distributions/ollama/trace_store.db
    provider_id: meta-reference
    provider_type: inline::meta-reference
  tool_runtime:
  - provider_id: model-context-protocol
    provider_type: remote::model-context-protocol
    config: {}
  - provider_id: rag-runtime 
    provider_type: inline::rag-runtime
    config: {}
  vector_io:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/faiss_store.db
        namespace: null
        type: sqlite
    provider_id: faiss
    provider_type: inline::faiss
  - provider_id: rhdh-docs 
    provider_type: inline::faiss
    config:
      kvstore:
        type: sqlite
        namespace: null
        db_path: /app-root/vector_db/rhdh_product_docs/1.7/faiss_store.db
scoring_fns: []
server:
  auth: null
  host: null
  port: 8321
  quota: null
  tls_cafile: null
  tls_certfile: null
  tls_keyfile: null
shields:
  - shield_id: lightspeed_question_validity-shield
    provider_id: lightspeed_question_validity
tool_groups:
- provider_id: rag-runtime
  toolgroup_id: builtin::rag
  description: "Only use for questions specifically about Red Hat Developer Hub (RHDH). Searches technical documentation for RHDH installation, discovery, configuration, release, upgrade, control access, integration, observability, and extending with plugins. Do not use for any other topic outside RHDH."
vector_dbs:
- embedding_dimension: 768
  embedding_model: sentence-transformers/all-mpnet-base-v2
  provider_id: rhdh-docs
  vector_db_id: rhdh-product-docs-1_7
----
where:
`_<llm_server>_`:: Enter your LLM server details.
`_<openai_api_key>_`:: Enter your OpenAI API key.

. Create the {ls-short} ConfigMap.
+
[NOTE]
====
Create a dedicated {ls-short} ConfigMap instead of adding an additional section to your existing {product-very-short} custom application configuration file (for example, `lightspeed-app-config.yaml`). Creating two files prevents the entire {product-very-short} ConfigMap from being loaded into {lcs-short}.
====
.. In the {ocp-short} web console, go to your {product-very-short} instance and select the *ConfigMaps* tab.
.. Click *Create ConfigMap*.
.. From the *Create ConfigMap* page, select the *YAML view* option in *Configure via*, and add the following example:
+
[source,yaml,subs="+attributes"]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: lightspeed-app-config
  namespace: <__namespace__> # Enter your {product-very-short} instance namespace
data:
  app-config.yaml: |-
    backend:
      csp:
         upgrade-insecure-requests: false
       img-src:
          - "'self'"
          - "data:"
          - https://img.freepik.com
          - https://cdn.dribbble.com
          - https://avatars.githubusercontent.com # This is to load GitHub avatars in the UI
       script-src:
           - "'self'"
           - https://cdn.jsdelivr.net

    lightspeed:
      mcpServers:
      - name: mcp::backstage
        token: ${MCP_TOKEN} 

      # OPTIONAL: Custom users prompts displayed to users
      # If not provided, the plugin uses built-in default prompts
      prompts:
        - title: 'Getting Started with {product}'
          message: Can you guide me through the first steps to start using {product-short} as a developer, like exploring the Software Catalog and adding my service?

      # OPTIONAL: Port for lightspeed service (default: 8080)
      # servicePort: ${LIGHTSPEED_SERVICE_PORT}

      # OPTIONAL: Override default RHDH system prompt
      # systemPrompt: "You are a helpful assistant focused on {product} development."
----
where:

`mcp_servers`:: Optional: Set this configuration if you want to integrate MCP.

. Create {ls-short} secret file.
.. In the {ocp-short} web console, go to *Secrets*.
.. Click *Create > Key/value secret*.
.. In the *Create key/value secret* page, select the *YAML view* option in *Configure via*, and add the following example:
+
[source,yaml]
----
kind: Secret
apiVersion: v1
metadata:
  name: lightspeed-secrets
  namespace: _<namespace>_ # Enter your rhdh instance namespace
stringData:
  LLM_SERVER_ID: _<server_id>_ # Enter your server ID (for example, `ollama` or `granite`)
  LLM_SERVER_TOKEN: _<token>_ # Enter your server token value
  LLM_SERVER_URL: _<server_url>_ # Enter your server URL
type: Opaque
----
.. Click *Create*.

. Create the Llama Stack secret file.
.. In the {ocp-short} web console, go to *Secrets*.
.. Click *Create > Key/value secret*.
.. In the *Create key/value secret* page, select the *YAML view* option in *Configure via*, and add the following example:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
    name: llama-stack-secrets
type: Opaque
stringData:
    VLLM_URL: "" # Set this if you are using redhat-ai-dev Llama Stack image
    VLLM_API_KEY: "" # Set this if you are using redhat-ai-dev Llama Stack
    VLLM_MAX_TOKENS: "" # Optional
    VLLM_TLS_VERIFY: "" # Optional
    OLLAMA_URL: "" # Set if you altered the run.yaml to use it
    OPENAI_API_KEY: "" # Set if you altered the run.yaml to use it
    VALIDATION_PROVIDER: "" # One of vllm, ollama, openai depending on above
    VALIDATION_MODEL_NAME: "" # Name of model you want used for validation
----
.. Click *Create*.

. To your existing dynamic plugins ConfigMap (for example, `dynamic-plugins-rhdh.yaml`), add the {ls-short} plugin image as shown in the following example:
+
[source,yaml,subs="+attributes"]
----
includes:
  - dynamic-plugins.default.yaml
 plugins:
  - package: oci://ghcr.io/redhat-developer/rhdh-plugin-export-overlays/red-hat-developer-hub-backstage-plugin-lightspeed:bs_1.39.1__0.5.7!red-hat-developer-hub-backstage-plugin-lightspeed
    disabled: false
    pluginConfig:
      lightspeed:
        # OPTIONAL: Custom users prompts displayed to users
        # If not provided, the plugin uses built-in default prompts
        prompts:
          - title: 'Getting Started with {product}'
            message: Can you guide me through the first steps to start using {product-short}
              as a developer, like exploring the Software Catalog and adding my
              service?
      dynamicPlugins:
        frontend:
          red-hat-developer-hub.backstage-plugin-lightspeed:
            appIcons:
              - name: LightspeedIcon
                module: LightspeedPlugin
                importName: LightspeedIcon
            dynamicRoutes:
              - path: /lightspeed
                importName: LightspeedPage
                module: LightspeedPlugin
                menuItem:
                  icon: LightspeedIcon
                  text: Lightspeed
  - package: oci://ghcr.io/redhat-developer/rhdh-plugin-export-overlays/red-hat-developer-hub-backstage-plugin-lightspeed-backend:bs_1.39.1__0.5.7!red-hat-developer-hub-backstage-plugin-lightspeed-backend
    disabled: false
    pluginConfig:
      lightspeed:
        # REQUIRED: Configure LLM servers with OpenAI API compatibility
        servers:
          - id: ${LLM_SERVER_ID}
            url: ${LLM_SERVER_URL}
            token: ${LLM_SERVER_TOKEN}

        # OPTIONAL: Port for lightspeed service (default: 8080)
        # servicePort: ${LIGHTSPEED_SERVICE_PORT}
----

. Update your deployment configuration based on your installation method:
.. For an Operator-installed {product-very-short} instance, update your {product-custom-resource-type} custom resource (CR).
... In the `spec.application.appConfig.configMaps` section, add the {ls-short} and Llama Stack custom app configurations as shown in the following example:
+
[source,yaml]
----
      appConfig:
        configMaps:
          - name: lightspeed-app-config
        mountPath: /opt/app-root/src
        configMaps:
          - name: llama-stack
        mountPath: /opt/app-root/src
----
... Update the `extraVolumes` specification to include the {lcs-short} and Llama Stack ConfigMaps as shown in the following example:
+
[source,yaml]
----
            volumes:
              - configMap:
                  name: lightspeed-stack
              - configMap:
                  name: llama-stack
----
... Update the `volumeMounts` specification to mount the {lcs-short} and Llama Stack ConfigMaps as shown in the following example:
+
[source,yaml]
----
        volumeMounts:
          - mountPath: /app-root/config/lightspeed-stack.yaml
            name: lightspeed-stack
            subPath: lightspeed-stack.yaml
          - mountPath: /app-root/config/app-config-rhdh.yaml
            name: lightspeed-app-config
            subPath: app-config.yaml
          - mountPath: /app-root-config/llama-stack/run.yaml
            subPath: llama-stack/run.yaml
----
... Add the {ls-short} and Llama Stack Secret file as shown in the following example:
+
[source,yaml]
----
        envFrom:
          - secretRef:
              name: lightspeed-secrets
          - secretRef:
              name: llama-stack-secrets
----
... In your `deployment.patch.spec.template.spec.containers.env` section, set the {lcs-short} and Llama Stack environment variables as shown in the following example:
+
[source,yaml]
----
    - name: PROJECT
      value: rhdh
    - name: LLAMA_CONFIG_FILE
      value: /app-root/config/llama-stack/run.yaml
----
+
[NOTE]
====
* Your {product-very-short} container is typically already present in your CR. You are adding the two additional container definitions for {lcs-short} and Llama Stack as the {lcs-short} sidecar.
====
... Click *Save*. The Pods are automatically restarted.
+
.Example of a Backstage CR with the {lcs-short} and Llama Stack container
[source,yaml,subs=+attributes]
----
kind: Backstage
metadata:
...
name: name
namespace: namespace
spec:
 deployment:
   patch:
     spec:
       template:
         spec:
           initContainers:
             - name: init-rag-data
               image: 'quay.io/redhat-ai-dev/rag-content:release-1.7-lcs'
               command:
                 - "sh"
                 - "-c"
                 - "echo 'Copying RAG data...'; cp -r /rag/vector_db/rhdh_product_docs /data/ && cp -r /rag/embeddings_model /data/ && echo 'Copy complete.'"
               volumeMounts:
                 - mountPath: /data
                   name: rag-data-volume
           containers:
             - envFrom:
               - secretRef:
                   name: llama-stack-secrets
               image: 'quay.io/redhat-ai-dev/llama-stack:latest'
               name: llama-stack
               volumeMounts:
                 - mountPath: /app-root/.llama
                   name: shared-storage
                 - mountPath: /app-root/embeddings_model
                   name: rag-data-volume
                   subPath: embeddings_model
                 - mountPath: /app-root/vector_db/rhdh_product_docs
                   name: rag-data-volume
                   subPath: rhdh_product_docs
             - image: <your-lcs-image>
               name: lightspeed-core
               volumeMounts:
                 - mountPath: /app-root/lightspeed-stack.yaml
                   name: lightspeed-stack
                   subPath: lightspeed-stack.yaml
                 - mountPath: /tmp/data/feedback
                   name: shared-storage
           volumes:
             - configMap:
                 name: lightspeed-stack
               name: lightspeed-stack
             - emptyDir: {}
               name: shared-storage
             - emptyDir: {}
               name: rag-data-volume
----
.. For a Helm-installed {product-very-short} instance, update your Helm chart.
... Add your dynamic plugins configuration in the`global.dynamic` property as shown in the following example:
+
[source,yaml,subs="+attributes"]
----
global:
dynamic:
  includes:
  - dynamic-plugins.default.yaml
  plugins:
  - package: oci://ghcr.io/redhat-developer/rhdh-plugin-export-overlays/red-hat-developer-hub-backstage-plugin-lightspeed:bs_1.39.1__0.5.7!red-hat-developer-hub-backstage-plugin-lightspeed
    disabled: false
    pluginConfig:
      lightspeed:
        # OPTIONAL: Custom users prompts displayed to users
        # If not provided, the plugin uses built-in default prompts
        prompts:
          - title: 'Getting Started with {product}'
            message: Can you guide me through the first steps to start using {product-short}
              as a developer, like exploring the Software Catalog and adding my
              service?
      dynamicPlugins:
        frontend:
          red-hat-developer-hub.backstage-plugin-lightspeed:
            appIcons:
              - name: LightspeedIcon
                module: LightspeedPlugin
                importName: LightspeedIcon
            dynamicRoutes:
              - path: /lightspeed
                importName: LightspeedPage
                module: LightspeedPlugin
                menuItem:
                  icon: LightspeedIcon
                  text: Lightspeed
  - package: oci://ghcr.io/redhat-developer/rhdh-plugin-export-overlays/red-hat-developer-hub-backstage-plugin-lightspeed-backend:bs_1.39.1__0.5.7!red-hat-developer-hub-backstage-plugin-lightspeed-backend
    disabled: false
    pluginConfig:
      lightspeed:
        # REQUIRED: Configure LLM servers with OpenAI API compatibility
        servers:
          - id: ${LLM_SERVER_ID}
            url: ${LLM_SERVER_URL}
            token: ${LLM_SERVER_TOKEN}
        # OPTIONAL: Port for lightspeed service (default: 8080)
        # servicePort: ${LIGHTSPEED_SERVICE_PORT}
----
... Add your {ls-short} custom app config file as shown in the following example:
+
[source,yaml]
----
 extraAppConfig:
      - configMapRef: lightspeed-app-config
        filename: app-config.yaml
----
... Update the `extraVolumes` section to include the {lcs-short} ConfigMap as shown in the following example:
+
[source,yaml]
----
extraVolumes:
      - configMap:
          name: /app-root/config/lightspeed-stack.yaml
        name: lightspeed-stack
----
... Update the `extraVolumeMounts` section to mount the {lcs-short} and Llama Stack ConfigMap as shown in the following example:
+
[source,yaml]
----
 extraVolumeMounts:
      - mountPath: /app-root/config/lightspeed-stack.yaml
        name: lightspeed-stack
----
... Add the Llama Stack Secret file as shown in the following example:
+
[source,yaml]
----
 extraEnvVarsSecrets:
      - llama-stack-secrets
----
... Add the {lcs-short} image as shown in the following example:
+
[source,yaml,subs="+attributes"]
----
   extraContainers:
      - env:
          - name: PROJECT
            value: rhdh
        envFrom:
          - secretRef:
              name: lightspeed-secrets
        image: 'quay.io/redhat-ai-dev/llama-stack:latest'
        name: road-core-sidecar
        ports:
          - containerPort: 8080
            name: lcs-backend
            protocol: TCP
        volumeMounts:
          - mountPath: /app-root/config/lightspeed-stack.yaml
            name: lightspeed-stack
            subPath: lightspeed-stack.yaml
          - mountPath: /app-root/run.yaml
            name: llama-stack-config
            subPath: run.yaml
----
+
[NOTE]
====
Your {product-very-short} container is typically already present in your Helm chart. You are adding the two additional container definitions for {lcs-short} and Llama Stack as the {lcs-short} sidecar.
====
... Click *Save*.
... Click *Helm upgrade*.
+
.Example of a Helm chart with the LCS and Llama Stack container
[source,yaml,subs="+attributes"]
----
global:
 ...
upstream:
 backstage:
   appConfig:
     ...
   args:
     ...
   extraAppConfig:
     - configMapRef: lightspeed-app-config
       filename: app-config.yaml
     - configMapRef: llama-stack-config
       filename: run.yaml
   extraContainers:
     - env:
         - name: PROJECT
           value: rhdh
         - name: LLAMA_STACK_CONFIG_FILE
           value: /app-root/run.yaml
       envFrom:
         - secretRef:
             name: lightspeed-secrets
       image: 'quay.io/redhat-ai-dev/llama-stack:latest'
       name: lightspeed-core-sidecar
       ports:
         - containerPort: 8080
           name: lcs-backend
           protocol: TCP
       volumeMounts:
         - mountPath: /app-root/config/lightspeed-stack.yaml
           name: lightspeed-stack
           subPath: lightspeed-stack.yaml
         - mountPath: /app-root/run.yaml
           name: llama-stack-config
           subPath: run.yaml
   extraEnvVars:
     ...
   extraEnvVarsSecrets:
     - llama-stack-secrets
   extraVolumeMounts:
     - mountPath: /app-root/config/lightspeed-stack.yaml
       name: lightspeed-stack
     - mountPath: /app-root/run.yaml
       name: llama-stack-config
   extraVolumes:
     - configMap:
         name: lightspeed-stack
       name: lightspeed-stack
     ...
   image:
     ...
   initContainers:
     ...
----

. Define permissions and roles for your users who are not administrators by completing the following steps:
.. Configure the required RBAC permission by defining an `rbac-policies.csv` file as shown in the following example:
+
[source,yaml]
----
p, role:default/_<your_team>_, lightspeed.chat.read, read, allow
p, role:default/_<your_team>_, lightspeed.chat.create, create, allow
p, role:default/_<your_team>_, lightspeed.chat.delete, delete, allow

g, user:default/_<your_user>_, role:default/_<your_team>_
----
.. Upload your `rbac-policies.csv` and `rbac-conditional-policies.yaml` files to an `rbac-policies` config map in your {ocp-short} project containing {product-very-short}.
.. Update your {product-custom-resource-type} custom resource to mount in the {product-very-short} filesystem your files from the `rbac-policies` ConfigMap:
+
[source,yaml]
----
apiVersion: rhdh.redhat.com/v1alpha3
kind: Backstage
spec:
  application:
    extraFiles:
      mountPath: /opt/app-root/src
      configMaps:
        - name: rbac-policies
----
For detailed information, see {authorization-book-link}managing-authorizations-by-using-external-files[Managing authorizations by using external files].

.Verification

. Log in to your {product-very-short} instance.
. In your {product} navigation menu, you are able to see and access the *Lightspeed* menu item. Clicking this menu takes you to the {ls-short} screen.

image::rhdh-plugins-reference/developer-lightspeed.png[]
