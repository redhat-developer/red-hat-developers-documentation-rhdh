:_mod-docs-content-type: PROCEDURE

[id="proc-changing-your-llm-provider_{context}"]
= Changing your LLM provider in {ls-short}

{ls-short} uses the {developer-lightspeed-link}#con-about-bring-your-own-model_appendix-about-user-data-security[_Bring Your Own Model_] approach. You must provide an LLM provider that conforms to the OpenAI API specification to enable chat services. 

Llama Stack acts as an intermediary layer that handles the configuration and setup of these LLM providers. The Llama Stack server can be configured to integrate with various LLM providers, including OpenAI, {rhoai-brand-name}, {rhel} AI, Ollama, and vLLM, provided they offer compatibility with the OpenAI API specification for chat completions.

Llama Stack references the environment variables (for example, `VLLM_URL` and `VLLM_API_KEY`) defined in the `llama-stack-secrets` Secret file to configure the remote provider.

.Procedure

* Define your new LLM provider by updating the `inference` section of the Llama Stack application configuration file `llama-stack.yaml`, as shown in the following example. 
+
[source,yaml]
----
  #START - Adding your LLM provider
  inference:
    - provider_id: vllm
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL}
        api_token: ${env.VLLM_API_KEY}
        max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
        tls_verify: ${env.VLLM_TLS_VERIFY:=true}
    - provider_id: sentence-transformers
      provider_type: inline::sentence-transformers
      config: {}
#END - Adding your LLM provider
----