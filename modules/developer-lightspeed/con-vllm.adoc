:_mod-docs-content-type: CONCEPT
[id="con-vllm_{context}"]
= vLLM

link:https://docs.vllm.ai/en/stable/[vLLM] is an open-source, high-throughput serving engine for large language models (LLMs) that significantly improves upon traditional serving systems. It achieves this by introducing several key optimizations to reduce memory usage and eliminate redundant computations. vLLM prominently increases the number of concurrent requests an LLM can handle, making it a powerful tool for deploying and scaling LLM-based applications.