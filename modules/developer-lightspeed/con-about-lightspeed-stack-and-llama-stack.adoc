:_mod-docs-content-type: CONCEPT

[id="con-about-lightspeed-stack-and-llama-stack_{context}"]
= About {lcs-name} and Llama Stack

The {lcs-name} and Llama Stack deploy together as sidecar containers to augment {product-very-short} functionality.

The Llama Stack delivers the augmented functionality by integrating and managing core components, which include:

* Large language model (LLM) inference providers

* Model Context Protocol (MCP) or Retrieval Augmented Generation (RAG) tool runtime providers
+
[IMPORTANT]
====
You must verify that your model supports tool calling before you enable Model Context Protocol (MCP) features. Using an incompatible model results in error messages.
====

* Safety providers

* Vector database settings

The {lcs-name} serves as the Llama Stack service intermediary. It manages the operational configuration and key data, specifically:

* User feedback collection

* MCP server configuration

* Conversation history

Llama Stack provides the inference functionality that {lcs-short} uses to process requests. For more information, see https://llamastack.github.io/docs#what-is-llama-stack[What is Llama Stack].

The {ls-brand-name} plugin in {product-very-short} sends prompts and receives LLM responses through the {lcs-short} sidecar. {lcs-short} then uses the Llama Stack sidecar service to perform inference and MCP or RAG tool calling.

[NOTE]
====
{ls-brand-name} is a Developer Preview release. You must manually deploy the {lcs-name} and Llama Stack sidecar containers, and install the {ls-brand-name} plugin on your {product-very-short} instance.
====