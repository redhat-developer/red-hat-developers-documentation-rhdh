<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>Audit log</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.2"/><meta name="description" content="As a Red Hat Developer Hub administrator, you can track user activities, system events, and data changes with Developer Hub audit logs."/><link rel="next" href="#assembly-audit-log" title="1. Audit logs in Red Hat Developer Hub"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><section xml:lang="en-US" class="article" id="idm45327499735984"><div class="titlepage"><div><div class="producttitle"><span class="productname">Red Hat Developer Hub</span> <span class="productnumber">1.4</span></div><div><h1 class="title">Audit log</h1></div><div><h3 class="subtitle"><em>Tracking user activities, system events, and data changes with Red Hat Developer Hub audit logs</em></h3></div><div><div xml:lang="en-US" class="authorgroup"><span class="orgname">Red Hat Customer Content Services</span></div></div><div><a href="#idm45327479157184">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				As a Red Hat Developer Hub administrator, you can track user activities, system events, and data changes with Developer Hub audit logs.
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="section"><a href="#assembly-audit-log">1. Audit logs in Red Hat Developer Hub</a></span></li><li><span class="section"><a href="#con-audit-log-config_assembly-audit-log">2. Configuring audit logs for Developer Hub on OpenShift Container Platform</a></span><ul><li><span class="section"><a href="#proc-forward-audit-log-splunk_assembly-audit-log">2.1. Forwarding Red Hat Developer Hub audit logs to Splunk</a></span></li></ul></li><li><span class="section"><a href="#proc-audit-log-view_assembly-audit-log">3. Viewing audit logs in Developer Hub</a></span><ul><li><span class="section"><a href="#ref-audit-log-fields.adoc_assembly-audit-log">3.1. Audit log fields</a></span></li><li><span class="section"><a href="#ref-audit-log-scaffolder-events.adoc_assembly-audit-log">3.2. Scaffolder events</a></span></li><li><span class="section"><a href="#ref-audit-log-catalog-events.adoc_assembly-audit-log">3.3. Catalog events</a></span></li></ul></li></ul></div><section class="section" id="assembly-audit-log"><div class="titlepage"><div><div><h2 class="title">1. Audit logs in Red Hat Developer Hub</h2></div></div></div><p>
			Audit logs are a chronological set of records documenting the user activities, system events, and data changes that affect your Red Hat Developer Hub users, administrators, or components. Administrators can view Developer Hub audit logs in the OpenShift Container Platform web console to monitor scaffolder events, changes to the RBAC system, and changes to the Catalog database. Audit logs include the following information:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					Name of the audited event
				</li><li class="listitem">
					Actor that triggered the audited event, for example, terminal, port, IP address, or hostname
				</li><li class="listitem">
					Event metadata, for example, date, time
				</li><li class="listitem">
					Event status, for example, <code class="literal">success</code>, <code class="literal">failure</code>
				</li><li class="listitem">
					Severity levels, for example, <code class="literal">info</code>, <code class="literal">debug</code>, <code class="literal">warn</code>, <code class="literal">error</code>
				</li></ul></div><p>
			You can use the information in the audit log to achieve the following goals:
		</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Enhance security</span></dt><dd>
						Trace activities, including those initiated by automated systems and software templates, back to their source. Know when software templates are executed, as well as the details of application and component installations, updates, configuration changes, and removals.
					</dd><dt><span class="term">Automate compliance</span></dt><dd>
						Use streamlined processes to view log data for specified points in time for auditing purposes or continuous compliance maintenance.
					</dd><dt><span class="term">Debug issues</span></dt><dd>
						Use access records and activity details to fix issues with software templates or plugins.
					</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
				Audit logs are not forwarded to the internal log store by default because this does not provide secure storage. You are responsible for ensuring that the system to which you forward audit logs is compliant with your organizational and governmental regulations, and is properly secured.
			</p></div></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
					For more information about logging in OpenShift Container Platform, see <a class="link" href="https://docs.openshift.com/container-platform/latest/observability/logging/cluster-logging.html">About Logging</a>
				</li><li class="listitem">
					For a complete list of fields that a Developer Hub audit log can include, see <a class="xref" href="#ref-audit-log-fields.adoc_assembly-audit-log" title="3.1. Audit log fields">Section 3.1, “Audit log fields”</a>
				</li><li class="listitem">
					For a list of scaffolder events that a Developer Hub audit log can include, see <a class="xref" href="#ref-audit-log-scaffolder-events.adoc_assembly-audit-log" title="3.2. Scaffolder events">Section 3.2, “Scaffolder events”</a>
				</li></ul></div></section><section class="section" id="con-audit-log-config_assembly-audit-log"><div class="titlepage"><div><div><h2 class="title">2. Configuring audit logs for Developer Hub on OpenShift Container Platform</h2></div></div></div><p>
			Use the OpenShift Container Platform web console to configure the following OpenShift Container Platform logging components to use audit logging for Developer Hub:
		</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Logging deployment</span></dt><dd>
						Configure the logging environment, including both the CPU and memory limits for each logging component. For more information, see <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html-single/logging/index#cluster-logging-memory">Red Hat OpenShift Container Platform - Configuring your Logging deployment</a>.
					</dd><dt><span class="term">Logging collector</span></dt><dd>
						Configure the <code class="literal">spec.collection</code> stanza in the <code class="literal">ClusterLogging</code> custom resource (CR) to use a supported modification to the log collector and collect logs from <code class="literal">STDOUT</code>. For more information, see <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html-single/logging/index#cluster-logging-collector">Red Hat OpenShift Container Platform - Configuring the logging collector</a>.
					</dd><dt><span class="term">Log forwarding</span></dt><dd>
						Send logs to specific endpoints inside and outside your OpenShift Container Platform cluster by specifying a combination of outputs and pipelines in a <code class="literal">ClusterLogForwarder</code> CR. For more information, see <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html-single/logging/index#cluster-logging-json-log-forwarding_cluster-logging-enabling-json-logging">Red Hat OpenShift Container Platform - Enabling JSON log forwarding</a> and <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html-single/logging/index#configuring-log-forwarding">Red Hat OpenShift Container Platform - Configuring log forwarding</a>.
					</dd></dl></div><section class="section" id="proc-forward-audit-log-splunk_assembly-audit-log"><div class="titlepage"><div><div><h3 class="title">2.1. Forwarding Red Hat Developer Hub audit logs to Splunk</h3></div></div></div><p>
				You can use the Red Hat OpenShift Logging (OpenShift Logging) Operator and a <code class="literal">ClusterLogForwarder</code> instance to capture the streamed audit logs from a Developer Hub instance and forward them to the HTTPS endpoint associated with your Splunk instance.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have a cluster running on a supported OpenShift Container Platform version.
					</li><li class="listitem">
						You have an account with <code class="literal">cluster-admin</code> privileges.
					</li><li class="listitem">
						You have a Splunk Cloud account or Splunk Enterprise installation.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Log in to your OpenShift Container Platform cluster.
					</li><li class="listitem"><p class="simpara">
						Install the OpenShift Logging Operator in the <code class="literal">openshift-logging</code> namespace and switch to the namespace:
					</p><div class="formalpara"><p class="title"><strong>Example command to switch to a namespace</strong></p><p>
							
<pre class="programlisting language-bash">oc project openshift-logging</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Create a <code class="literal">serviceAccount</code> named <code class="literal">log-collector</code> and bind the <code class="literal">collect-application-logs</code> role to the <code class="literal">serviceAccount</code> :
					</p><div class="formalpara"><p class="title"><strong>Example command to create a <code class="literal">serviceAccount</code></strong></p><p>
							
<pre class="programlisting language-bash">oc create sa log-collector</pre>
						</p></div><div class="formalpara"><p class="title"><strong>Example command to bind a role to a <code class="literal">serviceAccount</code></strong></p><p>
							
<pre class="programlisting language-bash">oc create clusterrolebinding log-collector --clusterrole=collect-application-logs --serviceaccount=openshift-logging:log-collector</pre>
						</p></div></li><li class="listitem">
						Generate a <code class="literal">hecToken</code> in your Splunk instance.
					</li><li class="listitem"><p class="simpara">
						Create a key/value secret in the <code class="literal">openshift-logging</code> namespace and verify the secret:
					</p><div class="formalpara"><p class="title"><strong>Example command to create a key/value secret with <code class="literal">hecToken</code></strong></p><p>
							
<pre class="programlisting language-bash">oc -n openshift-logging create secret generic splunk-secret --from-literal=hecToken=&lt;HEC_Token&gt;</pre>
						</p></div><div class="formalpara"><p class="title"><strong>Example command to verify a secret</strong></p><p>
							
<pre class="programlisting language-bash">oc -n openshift-logging get secret/splunk-secret -o yaml</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Create a basic `ClusterLogForwarder`resource YAML file as follows:
					</p><div class="formalpara"><p class="title"><strong>Example `ClusterLogForwarder`resource YAML file</strong></p><p>
							
<pre class="programlisting language-yaml">apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging</pre>
						</p></div><p class="simpara">
						For more information, see <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html-single/logging/index#logging-create-clf_configuring-log-forwarding">Creating a log forwarder</a>.
					</p></li><li class="listitem"><p class="simpara">
						Define the following <code class="literal">ClusterLogForwarder</code> configuration using OpenShift web console or OpenShift CLI:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Specify the <code class="literal">log-collector</code> as <code class="literal">serviceAccount</code> in the YAML file:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">serviceAccount</code> configuration</strong></p><p>
									
<pre class="programlisting language-yaml">serviceAccount:
  name: log-collector</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Configure <code class="literal">inputs</code> to specify the type and source of logs to forward. The following configuration enables the forwarder to capture logs from all applications in a provided namespace:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">inputs</code> configuration</strong></p><p>
									
<pre class="programlisting language-yaml">inputs:
  - name: my-app-logs-input
    type: application
    application:
      includes:
        - namespace: my-rhdh-project
      containerLimit:
        maxRecordsPerSecond: 100</pre>
								</p></div><p class="simpara">
								For more information, see <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html-single/logging/index#cluster-logging-collector-log-forward-logs-from-application-pods_configuring-log-forwarding">Forwarding application logs from specific pods</a>.
							</p></li><li class="listitem"><p class="simpara">
								Configure outputs to specify where the captured logs are sent. In this step, focus on the <code class="literal">splunk</code> type. You can either use <code class="literal">tls.insecureSkipVerify</code> option if the Splunk endpoint uses self-signed TLS certificates (not recommended) or provide the certificate chain using a Secret.
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">outputs</code> configuration</strong></p><p>
									
<pre class="programlisting language-yaml">outputs:
  - name: splunk-receiver-application
    type: splunk
    splunk:
      authentication:
        token:
          key: hecToken
          secretName: splunk-secret
      index: main
      url: 'https://my-splunk-instance-url'
      rateLimit:
        maxRecordsPerSecond: 250</pre>
								</p></div><p class="simpara">
								For more information, see <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html-single/logging/index#logging-forward-splunk_configuring-log-forwarding">Forwarding logs to Splunk</a> in OpenShift Container Platform documentation.
							</p></li><li class="listitem"><p class="simpara">
								Optional: Filter logs to include only audit logs:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">filters</code> configuration</strong></p><p>
									
<pre class="programlisting language-yaml">filters:
  - name: audit-logs-only
    type: drop
    drop:
      - test:
        - field: .message
          notMatches: isAuditLog</pre>
								</p></div><p class="simpara">
								For more information, see <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.16/html-single/logging/index#logging-content-filtering">Filtering logs by content</a> in OpenShift Container Platform documentation.
							</p></li><li class="listitem"><p class="simpara">
								Configure pipelines to route logs from specific inputs to designated outputs. Use the names of the defined inputs and outputs to specify multiple <code class="literal">inputRefs</code> and <code class="literal">outputRefs</code> in each pipeline:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">pipelines</code> configuration</strong></p><p>
									
<pre class="programlisting language-yaml">pipelines:
  - name: my-app-logs-pipeline
    detectMultilineErrors: true
    inputRefs:
      - my-app-logs-input
    outputRefs:
      - splunk-receiver-application
    filterRefs:
      - audit-logs-only</pre>
								</p></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Run the following command to apply the <code class="literal">ClusterLogForwarder</code> configuration:
					</p><div class="formalpara"><p class="title"><strong>Example command to apply <code class="literal">ClusterLogForwarder</code> configuration</strong></p><p>
							
<pre class="programlisting language-bash">oc apply -f &lt;ClusterLogForwarder-configuration.yaml&gt;</pre>
						</p></div></li><li class="listitem"><p class="simpara">
						Optional: To reduce the risk of log loss, configure your <code class="literal">ClusterLogForwarder</code> pods using the following options:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Define the resource requests and limits for the log collector as follows:
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">collector</code> configuration</strong></p><p>
									
<pre class="programlisting language-yaml">collector:
  resources:
    requests:
      cpu: 250m
      memory: 64Mi
      ephemeral-storage: 250Mi
    limits:
      cpu: 500m
      memory: 128Mi
      ephemeral-storage: 500Mi</pre>
								</p></div></li><li class="listitem"><p class="simpara">
								Define <code class="literal">tuning</code> options for log delivery, including <code class="literal">delivery</code>, <code class="literal">compression</code>, and <code class="literal">RetryDuration</code>. Tuning can be applied per output as needed.
							</p><div class="formalpara"><p class="title"><strong>Example <code class="literal">tuning</code> configuration</strong></p><p>
									
<pre class="programlisting language-yaml">tuning:
  delivery: AtLeastOnce <span id="CO1-1"/><span class="callout">1</span>
  compression: none
  minRetryDuration: 1s
  maxRetryDuration: 10s</pre>
								</p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										<code class="literal">AtLeastOnce</code> delivery mode means that if the log forwarder crashes or is restarted, any logs that were read before the crash but not sent to their destination are re-sent. It is possible that some logs are duplicated after a crash.
									</div></dd></dl></div></li></ol></div></li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Confirm that logs are being forwarded to your Splunk instance by viewing them in the Splunk dashboard.
					</li><li class="listitem">
						Troubleshoot any issues using OpenShift Container Platform and Splunk logs as needed.
					</li></ol></div></section></section><section class="section" id="proc-audit-log-view_assembly-audit-log"><div class="titlepage"><div><div><h2 class="title">3. Viewing audit logs in Developer Hub</h2></div></div></div><p>
			Administrators can view, search, filter, and manage the log data from the Red Hat OpenShift Container Platform web console. You can filter audit logs from other log types by using the <code class="literal">isAuditLog</code> field.
		</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
					You are logged in as an administrator in the OpenShift Container Platform web console.
				</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
					From the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console, click the <span class="strong strong"><strong>Topology</strong></span> tab.
				</li><li class="listitem">
					From the <span class="strong strong"><strong>Topology</strong></span> view, click the pod that you want to view audit log data for.
				</li><li class="listitem">
					From the pod panel, click the <span class="strong strong"><strong>Resources</strong></span> tab.
				</li><li class="listitem">
					From the <span class="strong strong"><strong>Pods</strong></span> section of the <span class="strong strong"><strong>Resources</strong></span> tab, click <span class="strong strong"><strong>View logs</strong></span>.
				</li><li class="listitem">
					From the <span class="strong strong"><strong>Logs</strong></span> view, enter <code class="literal">isAuditLog</code> into the <span class="strong strong"><strong>Search</strong></span> field to filter audit logs from other log types. You can use the arrows to browse the logs containing the <code class="literal">isAuditLog</code> field.
				</li></ol></div><section class="section" id="ref-audit-log-fields.adoc_assembly-audit-log"><div class="titlepage"><div><div><h3 class="title">3.1. Audit log fields</h3></div></div></div><p>
				Developer Hub audit logs can include the following fields:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">eventName</code></span></dt><dd>
							The name of the audited event.
						</dd><dt><span class="term"><code class="literal">actor</code></span></dt><dd><p class="simpara">
							An object containing information about the actor that triggered the audited event. Contains the following fields:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">actorId</code></span></dt><dd>
										The name/id/<code class="literal">entityRef</code> of the associated user or service. Can be <code class="literal">null</code> if an unauthenticated user accesses the endpoints and the default authentication policy is disabled.
									</dd><dt><span class="term"><code class="literal">ip</code></span></dt><dd>
										The IP address of the actor (optional).
									</dd><dt><span class="term"><code class="literal">hostname</code></span></dt><dd>
										The hostname of the actor (optional).
									</dd><dt><span class="term"><code class="literal">client</code></span></dt><dd>
										The user agent of the actor (optional).
									</dd></dl></div></dd><dt><span class="term"><code class="literal">stage</code></span></dt><dd>
							The stage of the event at the time that the audit log was generated, for example, <code class="literal">initiation</code> or <code class="literal">completion</code>.
						</dd><dt><span class="term"><code class="literal">status</code></span></dt><dd>
							The status of the event, for example, <code class="literal">succeeded</code> or <code class="literal">failed</code>.
						</dd><dt><span class="term"><code class="literal">meta</code></span></dt><dd>
							An optional object containing event specific data, for example, <code class="literal">taskId</code>.
						</dd><dt><span class="term"><code class="literal">request</code></span></dt><dd><p class="simpara">
							An optional field that contains information about the HTTP request sent to an endpoint. Contains the following fields:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">method</code></span></dt><dd>
										The HTTP method of the request.
									</dd><dt><span class="term"><code class="literal">query</code></span></dt><dd>
										The <code class="literal">query</code> fields of the request.
									</dd><dt><span class="term"><code class="literal">params</code></span></dt><dd>
										The <code class="literal">params</code> fields of the request.
									</dd><dt><span class="term"><code class="literal">body</code></span></dt><dd>
										The request <code class="literal">body</code>. The <code class="literal">secrets</code> provided when creating a task are redacted and appear as <code class="literal"><span class="strong strong"><strong>*</strong></span></code>.
									</dd><dt><span class="term"><code class="literal">url</code></span></dt><dd>
										The endpoint URL of the request.
									</dd></dl></div></dd><dt><span class="term"><code class="literal">response</code></span></dt><dd><p class="simpara">
							An optional field that contains information about the HTTP response sent from an endpoint. Contains the following fields:
						</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">status</code></span></dt><dd>
										The status code of the HTTP response.
									</dd><dt><span class="term"><code class="literal">body</code></span></dt><dd>
										The contents of the request body.
									</dd></dl></div></dd><dt><span class="term"><code class="literal">isAuditLog</code></span></dt><dd>
							A flag set to <code class="literal">true</code> to differentiate audit logs from other log types.
						</dd><dt><span class="term"><code class="literal">errors</code></span></dt><dd>
							A list of errors containing the <code class="literal">name</code>, <code class="literal">message</code> and potentially the <code class="literal">stack</code> field of the error. Only appears when <code class="literal">status</code> is <code class="literal">failed</code>.
						</dd></dl></div></section><section class="section" id="ref-audit-log-scaffolder-events.adoc_assembly-audit-log"><div class="titlepage"><div><div><h3 class="title">3.2. Scaffolder events</h3></div></div></div><p>
				Developer Hub audit logs can include the following scaffolder events:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">ScaffolderParameterSchemaFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/v2/templates/:namespace/:kind/:name/parameter-schema</code> endpoint which return template parameter schemas
						</dd><dt><span class="term"><code class="literal">ScaffolderInstalledActionsFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/v2/actions</code> endpoint which grabs the list of installed actions
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskCreation</code></span></dt><dd>
							Tracks <code class="literal">POST</code> requests to the <code class="literal">/v2/tasks</code> endpoint which creates tasks that the scaffolder executes
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskListFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/v2/tasks</code> endpoint which fetches details of all tasks in the scaffolder.
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/v2/tasks/:taskId</code> endpoint which fetches details of a specified task <code class="literal">:taskId</code>
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskCancellation</code></span></dt><dd>
							Tracks <code class="literal">POST</code> requests to the <code class="literal">/v2/tasks/:taskId/cancel</code> endpoint which cancels a running task
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskStream</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/v2/tasks/:taskId/eventstream</code> endpoint which returns an event stream of the task logs of task <code class="literal">:taskId</code>
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskEventFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/v2/tasks/:taskId/events</code> endpoint which returns a snapshot of the task logs of task <code class="literal">:taskId</code>
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskDryRun</code></span></dt><dd>
							Tracks <code class="literal">POST</code> requests to the <code class="literal">/v2/dry-run</code> endpoint which creates a dry-run task. All audit logs for events associated with dry runs have the <code class="literal">meta.isDryLog</code> flag set to <code class="literal">true</code>.
						</dd><dt><span class="term"><code class="literal">ScaffolderStaleTaskCancellation</code></span></dt><dd>
							Tracks automated cancellation of stale tasks
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskExecution</code></span></dt><dd>
							Tracks the <code class="literal">initiation</code> and <code class="literal">completion</code> of a real scaffolder task execution (will not occur during dry runs)
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskStepExecution</code></span></dt><dd>
							Tracks <code class="literal">initiation</code> and <code class="literal">completion</code> of a scaffolder task step execution
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskStepSkip</code></span></dt><dd>
							Tracks steps skipped due to <code class="literal">if</code> conditionals not being met
						</dd><dt><span class="term"><code class="literal">ScaffolderTaskStepIteration</code></span></dt><dd>
							Tracks the step execution of each iteration of a task step that contains the <code class="literal">each</code> field.
						</dd></dl></div></section><section class="section" id="ref-audit-log-catalog-events.adoc_assembly-audit-log"><div class="titlepage"><div><div><h3 class="title">3.3. Catalog events</h3></div></div></div><p>
				Developer Hub audit logs can include the following catalog events:
			</p><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">CatalogEntityAncestryFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/entities/by-name/:kind/:namespace/:name/ancestry</code> endpoint, which returns the ancestry of an entity
						</dd><dt><span class="term"><code class="literal">CatalogEntityBatchFetch</code></span></dt><dd>
							Tracks <code class="literal">POST</code> requests to the <code class="literal">/entities/by-refs</code> endpoint, which returns a batch of entities
						</dd><dt><span class="term"><code class="literal">CatalogEntityDeletion</code></span></dt><dd>
							Tracks <code class="literal">DELETE</code> requests to the <code class="literal">/entities/by-uid/:uid</code> endpoint, which deletes an entity
						</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					If the parent location of the deleted entity is still present in the catalog, then the entity is restored in the catalog during the next processing cycle.
				</p></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">CatalogEntityFacetFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/entity-facets</code> endpoint, which returns the facets of an entity
						</dd><dt><span class="term"><code class="literal">CatalogEntityFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/entities</code> endpoint, which returns a list of entities
						</dd><dt><span class="term"><code class="literal">CatalogEntityFetchByName</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/entities/by-name/:kind/:namespace/:name</code> endpoint, which returns an entity matching the specified entity reference, for example, <code class="literal">&lt;kind&gt;:&lt;namespace&gt;/&lt;name&gt;</code>
						</dd><dt><span class="term"><code class="literal">CatalogEntityFetchByUid</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/entities/by-uid/:uid</code> endpoint, which returns an entity matching the unique ID of the specified entity
						</dd><dt><span class="term"><code class="literal">CatalogEntityRefresh</code></span></dt><dd>
							Tracks <code class="literal">POST</code> requests to the <code class="literal">/entities/refresh</code> endpoint, which schedules the specified entity to be refreshed
						</dd><dt><span class="term"><code class="literal">CatalogEntityValidate</code></span></dt><dd>
							Tracks <code class="literal">POST</code> requests to the <code class="literal">/entities/validate</code> endpoint, which validates the specified entity
						</dd><dt><span class="term"><code class="literal">CatalogLocationCreation</code></span></dt><dd>
							Tracks <code class="literal">POST</code> requests to the <code class="literal">/locations</code> endpoint, which creates a location
						</dd></dl></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					A location is a marker that references other places to look for catalog data.
				</p></div></div><div class="variablelist"><dl class="variablelist"><dt><span class="term"><code class="literal">CatalogLocationAnalyze</code></span></dt><dd>
							Tracks <code class="literal">POST</code> requests to the <code class="literal">/locations/analyze</code> endpoint, which analyzes the specified location
						</dd><dt><span class="term"><code class="literal">CatalogLocationDeletion</code></span></dt><dd>
							Tracks <code class="literal">DELETE</code> requests to the <code class="literal">/locations/:id</code> endpoint, which deletes a location and all child entities associated with it
						</dd><dt><span class="term"><code class="literal">CatalogLocationFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/locations</code> endpoint, which returns a list of locations
						</dd><dt><span class="term"><code class="literal">CatalogLocationFetchByEntityRef</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/locations/by-entity</code> endpoint, which returns a list of locations associated with the specified entity reference
						</dd><dt><span class="term"><code class="literal">CatalogLocationFetchById</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/locations/:id</code> endpoint, which returns a location matching the specified location ID
						</dd><dt><span class="term"><code class="literal">QueriedCatalogEntityFetch</code></span></dt><dd>
							Tracks <code class="literal">GET</code> requests to the <code class="literal">/entities/by-query</code> endpoint, which returns a list of entities matching the specified query
						</dd></dl></div></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm45327479157184"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2025 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></section></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>