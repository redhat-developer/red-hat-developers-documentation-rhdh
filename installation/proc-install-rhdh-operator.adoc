[id='proc-install-rhdh-operator_{context}']
= Installing {product} using the Operator

[IMPORTANT]
====
These features are for Technology Preview only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs), might not be functionally complete, and Red Hat does not recommend using them for production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information on Red Hat Technology Preview features, see https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Scope].
====

== As an administrator

As an administrator, you can install {product} on your OpenShift Container Platform using the Operator.

.Prerequisites

* You are logged in as an administrator on the OpenShift web console.
* You have configured the appropriate roles and permissions within your project to create an application. See the link:https://docs.openshift.com/container-platform/4.14/applications/index.html[Red Hat OpenShift documentation on Building applications] for more details.

.Procedure

. In the *Administrator* perspective of the OpenShift web console, navigate to *Operators* → *OperatorHub*.

. Use the *Filter by keyword* box to search for the {product} Operator in the catalog, and then click the *Red Hat Developer Hub* tile.

. Install the {product} Operator. For more information, see link:https://docs.openshift.com/container-platform/4.14/operators/admin/olm-adding-operators-to-cluster.html#olm-installing-from-operatorhub-using-web-console_olm-adding-operators-to-a-cluster[Installing from OperatorHub using the web console]. 
+
[NOTE]
For enhanced security, you should deploy the Backstage Operator in a dedicated default namespace such as `rhdh-operator`. The cluster administrator can restrict other users' access to the operator resources through role bindings or cluster role bindings. You can choose to deploy the operator in the `openshift-operators` namespace instead, however, you should note that the {product} operator shares the namespace with other operators, and therefore any users who can create workloads in that namespace can get their privileges escalated from all operators' service accounts.

. See the “As a developer” section to continue setting up your {product} instance. 

== As a developer

As a developer, you can install {product} on your OpenShift Container Platform using the Operator.

.Prerequisites
* Your administrator has installed the {product} Operator. For more information see the xref:proc-install-rhdh-operator_{context}["As an administrator"] section.

.Procedure
. Create a project in OpenShift for your {product} instance. For more information about creating a project in OpenShift, see link:https://docs.openshift.com/container-platform/4.14/applications/projects/working-with-projects.html#odc-creating-projects-using-developer-perspective_projects[Red Hat OpenShift documentation].
. From the Developer perspective in the Red Hat OpenShift web console, click the *+Add* tab.
. From the *Developer Catalog* panel, click *Operator Backed*.
. Search for Developer Hub or Backstage in the search bar and select the *Developer Hub* card.
. Click *Create*.
. Optionally, configure the {product} Backstage instance with non-default settings.
. Click *Create*.
. From the *Topology* tab,  wait for the database and {product} to start.
. Click the *Open URL* option from the Developer Hub pod to start using the {product} platform.

image::rhdh/operator-install-1.png[]

== Configuring the Developer Hub Custom Resource

[NOTE]
Updates to the Backstage Custom Resource (CR) are automatically handled by the Operator. However, updates to resources referenced by the CR,  such as ConfigMaps or Secrets, are not updated automatically unless the CR itself is updated. If you want to update resources referenced by the CR, then you must manually delete the Backstage Deployment so that the Operator can re-create it with the updated resources.

=== Adding a custom application configuration file to Red Hat OpenShift
To change the configuration of your {product} instance, you must add a custom application configuration file to OpenShift and reference it in the Custom Resource. In OpenShift, you can use the following content as a base template to create a ConfigMap such as `app-config-rhdh.yaml`:

[source, yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: app-config-rhdh
data:
  app-config-rhdh.yaml: |
    app:
      title: Red Hat Developer Hub

    backend:
      auth:
        keys:
        - secret: “${BACKEND_SECRET}”
       baseUrl: https://backstage-<CUSTOM_RESOURCE_NAME>-<NAMESPACE_NAME>.<OPENSHIFT_INGRESS_DOMAIN>
       cors:
         origin: https://backstage-<CUSTOM_RESOURCE_NAME>-<NAMESPACE_NAME>.<OPENSHIFT_INGRESS_DOMAIN>
----

.Example
[source, yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: app-config-rhdh
data:
  "app-config-rhdh.yaml": |
    app:
      title: Red Hat Developer Hub
      baseUrl: https://backstage-developer-hub-my-ns.apps.ci-ln-vtkzr22-72292.origin-ci-int-gce.dev.rhcloud.com
    backend:
      auth:
        keys:
          - secret: "${BACKEND_SECRET}"
      baseUrl: https://backstage-backstage-sample-my-ns.apps.ci-ln-vtkzr22-72292.origin-ci-int-gce.dev.rhcloud.com
      cors:
        origin: https://backstage-backstage-sample-my-ns.apps.ci-ln-vtkzr22-72292.origin-ci-int-gce.dev.rhcloud.com
----

There is a mandatory Backend Auth Key for {product}. This references an environment variable defined in an OpenShift Secret.

[NOTE]
--
You are responsible for protecting your {product} installation from external and unauthorized access. The Backend Auth Key should be managed as any other secret. It should meet strong password requirements, you should not expose it in any configuration files and only inject it into configuration files as an environment variable.

For more information about roles and responsibilities in Developer Hub, see the xref:con-rbac-overview_{context}[Role-Based Access Control (RBAC) in {product}] section in the Administration Guide for {product}.
--

You need to know the external URL of your {product} instance and set it in the `app.baseUrl`, `backend.baseUrl` and `backend.cors.origin` fields of the application configuration. By default, this will be named as follows: `https://backstage-<CUSTOM_RESOURCE_NAME>-<NAMESPACE_NAME>.<OPENSHIFT_INGRESS_DOMAIN>;`. You can use the `oc get ingresses.config/cluster -o jsonpath='{.spec.domain}'` command to display your ingress domain. If you are using a different host or sub-domain, which is customizable in the `Custom Resource spec.application.route field`, you must adjust the application configuration accordingly.

.Prerequisites
* You have created an account in Red Hat OpenShift.

.Procedure
. From the *Developer* perspective, select the *ConfigMaps* tab.
. Click *Create ConfigMap*.
. Select the *YAML view* option in *Configure via* and make the changes to the file, if necessary.
. Click *Create*.
. Select the *Secrets* tab.
. Click *Create Key/value Secret*.
. Name the secret `secrets-rhdh`.
. Add a key named `BACKEND_SECRET` and a base64 encoded string as a value. Use a unique value for each {product} instance. For example, you can use the following command to generate a key from your terminal:
+
[source,yaml]
----
node -p 'require("crypto").randomBytes(24).toString("base64")'
----

. Click *Create*.
. Select the *Topology* tab.
. Click on the three dots menu of a {product} instance and select *Edit Backstage* to load the YAML view of the {product} instance.
+
image::rhdh/operator-install-2.png[]


. Add the `spec.application.appConfig.configMaps` and `spec.application.extraEnvs.secrets` fields to the Custom Resource, as follows:
+
[source, yaml]
----
spec:  application:
    appConfig:
      mountPath: /opt/app-root/src
      configMaps:
         - name: app-config-rhdh
    extraEnvs:
      secrets:
         - name: secrets-rhdh
    extraFiles:
      mountPath: /opt-/app-root/src
    replicas: 1
    route:
      enabled: true
  database:
enableLocalDb: true
----
. Click *Save*.
. Navigate back to the *Topology* view and wait for the {product} pod to start.
. Click the *Open URL* option to start using the {product} platform with the new configuration changes.

== Configuring dynamic plugins with the Operator
You can store the configuration for dynamic plugins in a ConfigMap object that the Custom Resource can reference.

.Example using the GitHub dynamic plugin
In OpenShift, you can use the following content as a base template to create a ConfigMap named `dynamic-plugins-rhdh`:

[source, yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: dynamic-plugins-rhdh
data:
  dynamic-plugins.yaml: |
    includes:
      - dynamic-plugins.default.yaml
    plugins:
      - package: './dynamic-plugins/dist/backstage-plugin-catalog-backend-module-github-dynamic'
        disabled: false
        pluginConfig: {}
----

[NOTE]
--
If the `pluginConfig` field references environment variables, you must define the variables in your `secrets-rhdh` secret.
--

.Procedure
. Navigate to OpenShift and select the *ConfigMaps* tab.
. Click *Create ConfigMap*.
+
The *Create ConfigMap* page appears.
. Select the *YAML view* option in *Configure via* and edit the file, if needed.
. Click *Create*.
. Go to the *Topology* tab.
. Click on the three dots menu of a {product} instance and select *Edit Backstage* to load the YAML view of the {product} instance.
+
image::rhdh/operator-install-2.png[]


. Add the `spec.application.dynamicPluginsConfigMapName` field to the Custom Resource, as follows:
+
[source,yaml]
----
spec:
  application:
    appConfig:
      mountPath: /opt/app-root/src
      configMaps:
        - name: app-config-rhdh
    dynamicPluginsConfigMapName: dynamic-plugins-rhdh
    extraEnvs:
      secrets:
        - name: secrets-rhdh
    extraFiles:
      mountPath: /opt-/app-root/src
    replicas: 1
    route:
      enabled: true
  database:
    enableLocalDb: true
----
. Click *Save*.
. Navigate back to the *Topology* view and wait for the {product} pod to start.
. Click the *Open URL* option to start using the {product} platform with the new configuration changes.

.Verification
To check that the dynamic plugins configuration has been loaded, append the following to your {product} root URL: `/api/dynamic-plugins-info/loaded-plugins` and check the list of plugins.

image::rhdh/operator-install-3.png[]

== Installing {product} Hub using a custom Backstage image
You can install {product} that uses a custom Backstage image in one of the following ways:

* Use the *Form view* and enter the image in *application* -> *image*
* Use the *YAML view* to enter the image directly in the Backstage Custom Resource specification, as follows:
[source,yaml]
----
spec:
  application:
       image: <your custom image>
----

[WARNING]
Installing a {product} application with a custom Backstage image might pose security risks to your organization. It is your responsibility to ensure that the image is from trusted sources, and has been tested and validated for security compliance. Red Hat only supports the images shipped within the {product} Operator.


== Installing {product} using the operator in an air-gapped environment
On an OpenShift cluster operating on a restricted network, public resources are not available. However, deploying the {product} (RHDH) Operator and running RHDH requires the following public resources:

* Operator images (bundle, operator, catalog)
* Operands images (RHDH, PostgreSQL)

To make these resources available, replace these resources with their equivalent resources in a mirror registry accessible to the OpenShift cluster.

You can use a helper script that mirrors the necessary images and provides the necessary configuration to ensure those images will be used when installing the RHDH Operator and creating RHDH instances.

[NOTE]
This script requires a target mirror registry which you should already have installed if your OpenShift cluster is ready to operate on a restricted network. However, if you are preparing your cluster for disconnected usage, you can use the script to deploy a mirror registry in the cluster and use it for the mirroring process.

.Prerequisites
* An active `oc` session with administrative permissions to the OpenShift cluster. See link:https://docs.openshift.com/container-platform/4.14/cli_reference/openshift_cli/getting-started-cli.html[Getting started with the OpenShift CLI].
* An active `oc registry` session to the `registry.redhat.io` Red Hat Ecosystem Catalog. See link:https://access.redhat.com/RegistryAuthentication[Red Hat Container Registry Authentication].
* The `opm` CLI tool is installed. See link:https://docs.openshift.com/container-platform/4.14/cli_reference/opm/cli-opm-install.html[Installing the opm CLI].
* The jq package is installed. See link:https://jqlang.github.io/jq/download/[Download jq].
* Podman is installed. See link:https://podman.io/docs/installation[Podman Installation Instructions].
* Skopeo version 1.14 or higher is installed. link:https://github.com/containers/skopeo/blob/main/install.md[See Installing Skopeo].
* If you already have a mirror registry for your cluster, an active Skopeo session with administrative access to this registry is required. See link:https://github.com/containers/skopeo#authenticating-to-a-registry[Authenticating to a registry] and link:https://docs.openshift.com/container-platform/4.14/installing/disconnected_install/installing-mirroring-installation-images.html[Mirroring images for a disconnected installation].

[NOTE]
The internal OpenShift cluster image registry cannot be used as a target mirror registry. See link:https://docs.openshift.com/container-platform/4.14/installing/disconnected_install/installing-mirroring-installation-images.html#installation-about-mirror-registry_installing-mirroring-installation-images[About the mirror registry].

* If you prefer to create your own mirror registry, see link:https://docs.openshift.com/container-platform/4.14/installing/disconnected_install/installing-mirroring-creating-registry.html[Creating a mirror registry with mirror registry for Red Hat OpenShift].

* If you do not already have a mirror registry, you can use the helper script to create one for you and you need the following additional prerequisites:
+
* The cURL package is installed. For Red Hat Enterprise Linux, the curl command is available by installing the curl package. To use curl for other platforms, see the link:https://curl.se/[cURL website].
* The `htpasswd` command is available. For Red Hat Enterprise Linux, the `htpasswd` command is available by installing the `httpd-tools` package.

.Procedure
. Download and run the mirroring script to install a custom Operator catalog and mirror the related images: `prepare-restricted-environment.sh` (link:https://github.com/janus-idp/operator/blob/1.1.x/.rhdh/scripts/prepare-restricted-environment.sh[source]).
+
[source,yaml]
----
# if you do not already have a target mirror registry
# and want the script to create one for you.
bash prepare-restricted-environment.sh \
   --prod_operator_index "registry.redhat.io/redhat/redhat-operator-index:v4.14" \
   --prod_operator_package_name "rhdh" \
   --prod_operator_bundle_name "rhdh-operator" \
   --prod_operator_version "v1.1.0"

# or, if you already have a target mirror registry
bash prepare-restricted-environment.sh \
   --prod_operator_index "registry.redhat.io/redhat/redhat-operator-index:v4.14" \
   --prod_operator_package_name "rhdh" \
   --prod_operator_bundle_name "rhdh-operator" \
   --prod_operator_version "v1.1.0" \
   --use_existing_mirror_registry "<my_registry>"
----
+
[NOTE]
The script can take several minutes to complete as it copies multiple images to the mirror registry.


. Refer to the xref:proc-install-rhdh-operator_{context}[Installing {product} using the operator as an administrator] section to install the operator and configure your {product} instance.

