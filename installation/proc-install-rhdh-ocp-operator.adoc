[id="proc-install-rhdh-ocp-operator_{context}"]
= Deploying {product} on {ocp-short} using the Operator

As a developer, you can deploy a {product} instance on {ocp-short} by using the *Developer* perspective in the web console.

.Prerequisites

* A cluster administrator has installed the {product} Operator. For more information, see xref:proc-install-operator_admin-rhdh[Installing the {product} Operator].

.Procedure

. Create a project in {ocp-short} for your {product} instance, or select an existing project.
+
[TIP]
====
For more information about creating a project in {ocp-short}, see link:https://docs.openshift.com/container-platform/4.15/applications/projects/working-with-projects.html#creating-a-project-using-the-web-console_projects[Creating a project by using the web console] in the {ocp-brand-name} documentation.
====
. From the *Developer* perspective on the {ocp-short} web console, click *+Add*.
. From the *Developer Catalog* panel, click *Operator Backed*.
. In the *Filter by keyword* box, enter _{product-short}_ and click the *{product}* card.
. Click *Create*.
. Optional. Add <<Configuring the {product-short} Custom Resource, custom configurations>> for the {product} instance.
. On the *Create Backstage* page, click *Create*

.Verification

After the pods are ready, you can access the {product} platform by opening the URL.

. Confirm that the pods are ready by clicking the pod in the the *Topology* tab and confirming the *Status* in the *Details* panel. The pod status is *Active* when the pod is ready.

. From the *Topology* tab, click the *Open URL* icon on the {product-short} pod.
+
image::rhdh/operator-install-1.png[]

[role="_additional-resources"]
[id="additional-resources_proc-install-rhdh-ocp-operator"]
.Additional resources
* link:https://docs.openshift.com/container-platform/{ocp-version}/applications/index.html[{ocp-short} - Building applications overview]

[id="proc-install-rhdh-ocp-operator-configuring-cr_{context}"]
== Configuring the {product-short} Custom Resource

[NOTE]
====
Updates to the Backstage Custom Resource (CR) are automatically handled by the {product} Operator. However, updates to resources referenced by the CR, such as ConfigMaps or Secrets, are not updated automatically unless the CR itself is updated. If you want to update resources referenced by the CR, then you must manually delete the Backstage Deployment so that the Operator can re-create it with the updated resources.
====

=== Adding a custom application configuration file to {ocp-short}
To change the configuration of your {product} instance, you must add a custom application configuration file to {ocp-short} and reference it in the Custom Resource. In {ocp-short}, you can use the following content as a base template to create a ConfigMap such as `app-config-rhdh.yaml`:

[source,yaml,subs="attributes+"]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: app-config-rhdh
data:
  app-config-rhdh.yaml: |
    app:
      title: {product}

    backend:
      auth:
        keys:
          - secret: “${BACKEND_SECRET}”
      baseUrl: https://backstage-<CUSTOM_RESOURCE_NAME>-<NAMESPACE_NAME>.<OPENSHIFT_INGRESS_DOMAIN>
      cors:
        origin: https://backstage-<CUSTOM_RESOURCE_NAME>-<NAMESPACE_NAME>.<OPENSHIFT_INGRESS_DOMAIN>
----

.Example
[source,yaml,subs="attributes+"]

----
kind: ConfigMap
apiVersion: v1
metadata:
  name: app-config-rhdh
data:
  "app-config-rhdh.yaml": |
    app:
      title: {product}
      baseUrl: https://backstage-developer-hub-my-ns.apps.ci-ln-vtkzr22-72292.origin-ci-int-gce.dev.rhcloud.com
    backend:
      auth:
        keys:
          - secret: "${BACKEND_SECRET}"
      baseUrl: https://backstage-backstage-sample-my-ns.apps.ci-ln-vtkzr22-72292.origin-ci-int-gce.dev.rhcloud.com
      cors:
        origin: https://backstage-backstage-sample-my-ns.apps.ci-ln-vtkzr22-72292.origin-ci-int-gce.dev.rhcloud.com
----

There is a mandatory Backend Auth Key for {product}. This references an environment variable defined in an {ocp-short} Secret.

[NOTE]
--
You are responsible for protecting your {product} installation from external and unauthorized access. The Backend Auth Key should be managed as any other secret. It should meet strong password requirements, you should not expose it in any configuration files and only inject it into configuration files as an environment variable.

For more information about roles and responsibilities in {product-short}, see the xref:con-rbac-overview_{context}[Role-Based Access Control (RBAC) in {product}] section in the Administration Guide for {product}.
--

You need to know the external URL of your {product} instance and set it in the `app.baseUrl`, `backend.baseUrl` and `backend.cors.origin` fields of the application configuration. By default, this will be named as follows: `pass:c[https://backstage-<CUSTOM_RESOURCE_NAME>-<NAMESPACE_NAME>.<OPENSHIFT_INGRESS_DOMAIN>;]`. You can use the `oc get ingresses.config/cluster -o jsonpath='{.spec.domain}'` command to display your ingress domain. If you are using a different host or sub-domain, which is customizable in the `Custom Resource spec.application.route field`, you must adjust the application configuration accordingly.

.Prerequisites
* You have created a {ocp-brand-name} account.

.Procedure
. From the *Developer* perspective, select the *ConfigMaps* tab.
. Click *Create ConfigMap*.
. Select the *YAML view* option in *Configure via* and make the changes to the file, if necessary.
. Click *Create*.
. Select the *Secrets* tab.
. Click *Create Key/value Secret*.
. Name the secret `secrets-rhdh`.
. Add a key named `BACKEND_SECRET` and a base64 encoded string as a value. Use a unique value for each {product} instance. For example, you can use the following command to generate a key from your terminal:
+
[source,yaml]
----
node -p 'require("crypto").randomBytes(24).toString("base64")'
----

. Click *Create*.
. Select the *Topology* tab.
. Click on the three dots menu of a {product} instance and select *Edit Backstage* to load the YAML view of the {product} instance.
+
image::rhdh/operator-install-2.png[]


. Add the `spec.application.appConfig.configMaps` and `spec.application.extraEnvs.secrets` fields to the Custom Resource, as follows:
+
[source, yaml]
----
spec:
  application:
    appConfig:
      mountPath: /opt/app-root/src
      configMaps:
         - name: app-config-rhdh
    extraEnvs:
      secrets:
         - name: secrets-rhdh
    extraFiles:
      mountPath: /opt/app-root/src
    replicas: 1
    route:
      enabled: true
  database:
    enableLocalDb: true
----
. Click *Save*.
. Navigate back to the *Topology* view and wait for the {product} pod to start.
. Click the *Open URL* option to start using the {product} platform with the new configuration changes.

== Configuring dynamic plugins with the {product} Operator
You can store the configuration for dynamic plugins in a ConfigMap object that the Custom Resource can reference.

.Example using the GitHub dynamic plugin
In {ocp-short}, you can use the following content as a base template to create a ConfigMap named `dynamic-plugins-rhdh`:

[source, yaml]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: dynamic-plugins-rhdh
data:
  dynamic-plugins.yaml: |
    includes:
      - dynamic-plugins.default.yaml
    plugins:
      - package: './dynamic-plugins/dist/backstage-plugin-catalog-backend-module-github-dynamic'
        disabled: false
        pluginConfig: {}
----

[NOTE]
--
If the `pluginConfig` field references environment variables, you must define the variables in your `secrets-rhdh` secret.
--

.Procedure
. From the {ocp-short} web console, select the *ConfigMaps* tab.
. Click *Create ConfigMap*.
. From the *Create ConfigMap* page, select the *YAML view* option in *Configure via* and edit the file, if needed.
. Click *Create*.
. Go to the *Topology* tab.
. Click on the overflow menu of a {product} instance and select *Edit Backstage* to load the YAML view of the {product} instance.
+
image::rhdh/operator-install-2.png[]


. Add the `spec.application.dynamicPluginsConfigMapName` field to the Custom Resource, as follows:
+
[source,yaml]
----
spec:
  application:
    appConfig:
      mountPath: /opt/app-root/src
      configMaps:
        - name: app-config-rhdh
    dynamicPluginsConfigMapName: dynamic-plugins-rhdh
    extraEnvs:
      secrets:
        - name: secrets-rhdh
    extraFiles:
      mountPath: /opt/app-root/src
    replicas: 1
    route:
      enabled: true
  database:
    enableLocalDb: true
----
. Click *Save*.
. Navigate back to the *Topology* view and wait for the {product} pod to start.
. Click the *Open URL* icon to start using the {product} platform with the new configuration changes.

.Verification
To check that the dynamic plugins configuration has been loaded, append the following to your {product} root URL: `/api/dynamic-plugins-info/loaded-plugins` and check the list of plugins.

image::rhdh/operator-install-3.png[]

== Installing {product} using a custom Backstage image
You can install {product} that uses a custom Backstage image in one of the following ways:

* Use the *Form view* and enter the image in *application* -> *image*
* Use the *YAML view* to enter the image directly in the Backstage Custom Resource specification, as follows:
[source,yaml]
----
spec:
  application:
       image: <your custom image>
----

[WARNING]
Installing a {product} application with a custom Backstage image might pose security risks to your organization. It is your responsibility to ensure that the image is from trusted sources, and has been tested and validated for security compliance. {company-name} only supports the images shipped within the {product} Operator.


== Installing {product} using the Operator in an air-gapped environment
On an {ocp-short} cluster operating on a restricted network, public resources are not available. However, deploying the {product} Operator and running {product-short} requires the following public resources:

* Operator images (bundle, operator, catalog)
* Operands images ({product-very-short}, PostgreSQL)

To make these resources available, replace these resources with their equivalent resources in a mirror registry accessible to the {ocp-short} cluster.

You can use a helper script that mirrors the necessary images and provides the necessary configuration to ensure those images will be used when installing the {product} Operator and creating {product-short} instances.

[NOTE]
This script requires a target mirror registry which you should already have installed if your {ocp-short} cluster is ready to operate on a restricted network. However, if you are preparing your cluster for disconnected usage, you can use the script to deploy a mirror registry in the cluster and use it for the mirroring process.

.Prerequisites
* An active `oc` session with administrative permissions to the {ocp-short} cluster. See link:https://docs.openshift.com/container-platform/{ocp-version}/cli_reference/openshift_cli/getting-started-cli.html[Getting started with the OpenShift CLI].
* An active `oc registry` session to the `registry.redhat.io` Red Hat Ecosystem Catalog. See link:https://access.redhat.com/RegistryAuthentication[{company-name} Container Registry Authentication].
* The `opm` CLI tool is installed. See link:https://docs.openshift.com/container-platform/{ocp-version}/cli_reference/opm/cli-opm-install.html[Installing the opm CLI].
* The jq package is installed. See link:https://jqlang.github.io/jq/download/[Download jq].
* Podman is installed. See link:https://podman.io/docs/installation[Podman Installation Instructions].
* Skopeo version 1.14 or higher is installed. link:https://github.com/containers/skopeo/blob/main/install.md[See Installing Skopeo].
* If you already have a mirror registry for your cluster, an active Skopeo session with administrative access to this registry is required. See link:https://github.com/containers/skopeo#authenticating-to-a-registry[Authenticating to a registry] and link:https://docs.openshift.com/container-platform/{ocp-version}/installing/disconnected_install/installing-mirroring-installation-images.html[Mirroring images for a disconnected installation].

[NOTE]
The internal {ocp-short} cluster image registry cannot be used as a target mirror registry. See link:https://docs.openshift.com/container-platform/{ocp-version}/installing/disconnected_install/installing-mirroring-installation-images.html#installation-about-mirror-registry_installing-mirroring-installation-images[About the mirror registry].

* If you prefer to create your own mirror registry, see link:https://docs.openshift.com/container-platform/{ocp-version}/installing/disconnected_install/installing-mirroring-creating-registry.html[Creating a mirror registry with mirror registry for Red Hat OpenShift].

* If you do not already have a mirror registry, you can use the helper script to create one for you and you need the following additional prerequisites:
+
* The cURL package is installed. For {rhel}, the curl command is available by installing the curl package. To use curl for other platforms, see the link:https://curl.se/[cURL website].
* The `htpasswd` command is available. For {rhel}, the `htpasswd` command is available by installing the `httpd-tools` package.

.Procedure
. Download and run the mirroring script to install a custom Operator catalog and mirror the related images: `prepare-restricted-environment.sh` (link:https://github.com/janus-idp/operator/blob/1.1.x/.rhdh/scripts/prepare-restricted-environment.sh[source]).
+
[source,yaml]
----
curl -sSLO https://raw.githubusercontent.com/janus-idp/operator/1.1.x/.rhdh/scripts/prepare-restricted-environment.sh

# if you do not already have a target mirror registry
# and want the script to create one for you.
bash prepare-restricted-environment.sh \
   --prod_operator_index "registry.redhat.io/redhat/redhat-operator-index:v4.14" \
   --prod_operator_package_name "rhdh" \
   --prod_operator_bundle_name "rhdh-operator" \
   --prod_operator_version "v1.1.1"

# or, if you already have a target mirror registry
bash prepare-restricted-environment.sh \
   --prod_operator_index "registry.redhat.io/redhat/redhat-operator-index:v4.14" \
   --prod_operator_package_name "rhdh" \
   --prod_operator_bundle_name "rhdh-operator" \
   --prod_operator_version "v1.1.1" \
   --use_existing_mirror_registry "my_registry"
----
+
[NOTE]
The script can take several minutes to complete as it copies multiple images to the mirror registry.
